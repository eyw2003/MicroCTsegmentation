{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f62cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch.cuda\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "from  segmentation_models_pytorch.utils.base import Metric\n",
    "from segmentation_models_pytorch.base.modules import Activation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a14e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47e8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    all_images=os.listdir(path/'images')\n",
    "    all_masks=os.listdir(path/'masks')\n",
    "    \n",
    "    data = {'images':[],\n",
    "           'masks':[]}\n",
    "    for i in range(len(all_images)):\n",
    "        data['images'].append(str(path/'images'/all_images[i]))\n",
    "        data['masks'].append(str(path/'masks'/all_masks[i]))\n",
    "    return pd.DataFrame(data)\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95911029",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_2D_BASE_PATH=Path(r'C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhammad\\Axials Version')\n",
    "WIDTH=320\n",
    "HEIGHT=320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e3064a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20588</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20589</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20590</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20591</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20592</th>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "      <td>C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20593 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  images  \\\n",
       "0      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "1      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "2      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "3      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "4      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "...                                                  ...   \n",
       "20588  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "20589  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "20590  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "20591  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "20592  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...   \n",
       "\n",
       "                                                   masks  \n",
       "0      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "1      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "2      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "3      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "4      C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "...                                                  ...  \n",
       "20588  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "20589  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "20590  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "20591  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "20592  C:\\Users\\lm3088\\Documents\\POM-CTproject\\Muhamm...  \n",
       "\n",
       "[20593 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=load_data(DATASET_2D_BASE_PATH/'train')\n",
    "df_val=load_data(DATASET_2D_BASE_PATH/'val')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c268cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_path, \n",
    "            masks_path, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.images = images_path\n",
    "        self.masks = masks_path\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "#         print(self.images[i])\n",
    "        image = cv2.imread(str(self.images[i]))\n",
    "        mask = cv2.imread(self.masks[i],0)\n",
    "        mask=np.expand_dims(mask,axis=-1)\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c50cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "   \n",
    "        albu.Resize(HEIGHT,WIDTH),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.2,p=1, border_mode=cv2.BORDER_CONSTANT),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.Blur(blur_limit=3, p=0.4),\n",
    "        albu.GaussNoise(p=0.5),\n",
    "        albu.RandomBrightnessContrast(brightness_limit=0.3,contrast_limit=0.3,p=0.5),\n",
    "        albu.RandomBrightness(p=0.75)\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "  \n",
    "    test_transform = [\n",
    "        albu.Resize(HEIGHT,WIDTH)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):  \n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b021bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to C:\\Users\\lm3088/.cache\\torch\\hub\\checkpoints\\resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfdba02e0b54322876f4959b59b12b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENCODER = 'efficientnet-b4' #'se_resnext50_32x4d' 'resnet101'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "TRAIN_RUNS_PATH=r'C:\\Users\\lm3088\\Documents\\GitHub\\MicroCTsegmentation\\runs'\n",
    "MODEL_NAME='Unet'\n",
    "BATCH_SIZE=32\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6730c8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lm3088\\\\Documents\\\\GitHub\\\\MicroCTsegmentation\\\\runs\\\\Unet_resnet101'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_PATH=os.path.join(TRAIN_RUNS_PATH,f'{MODEL_NAME}_{ENCODER}')\n",
    "WEIGHTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a0bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 160, 160]           9,408\n",
      "       BatchNorm2d-2         [32, 64, 160, 160]             128\n",
      "              ReLU-3         [32, 64, 160, 160]               0\n",
      "         MaxPool2d-4           [32, 64, 80, 80]               0\n",
      "            Conv2d-5           [32, 64, 80, 80]           4,096\n",
      "       BatchNorm2d-6           [32, 64, 80, 80]             128\n",
      "              ReLU-7           [32, 64, 80, 80]               0\n",
      "            Conv2d-8           [32, 64, 80, 80]          36,864\n",
      "       BatchNorm2d-9           [32, 64, 80, 80]             128\n",
      "             ReLU-10           [32, 64, 80, 80]               0\n",
      "           Conv2d-11          [32, 256, 80, 80]          16,384\n",
      "      BatchNorm2d-12          [32, 256, 80, 80]             512\n",
      "           Conv2d-13          [32, 256, 80, 80]          16,384\n",
      "      BatchNorm2d-14          [32, 256, 80, 80]             512\n",
      "             ReLU-15          [32, 256, 80, 80]               0\n",
      "       Bottleneck-16          [32, 256, 80, 80]               0\n",
      "           Conv2d-17           [32, 64, 80, 80]          16,384\n",
      "      BatchNorm2d-18           [32, 64, 80, 80]             128\n",
      "             ReLU-19           [32, 64, 80, 80]               0\n",
      "           Conv2d-20           [32, 64, 80, 80]          36,864\n",
      "      BatchNorm2d-21           [32, 64, 80, 80]             128\n",
      "             ReLU-22           [32, 64, 80, 80]               0\n",
      "           Conv2d-23          [32, 256, 80, 80]          16,384\n",
      "      BatchNorm2d-24          [32, 256, 80, 80]             512\n",
      "             ReLU-25          [32, 256, 80, 80]               0\n",
      "       Bottleneck-26          [32, 256, 80, 80]               0\n",
      "           Conv2d-27           [32, 64, 80, 80]          16,384\n",
      "      BatchNorm2d-28           [32, 64, 80, 80]             128\n",
      "             ReLU-29           [32, 64, 80, 80]               0\n",
      "           Conv2d-30           [32, 64, 80, 80]          36,864\n",
      "      BatchNorm2d-31           [32, 64, 80, 80]             128\n",
      "             ReLU-32           [32, 64, 80, 80]               0\n",
      "           Conv2d-33          [32, 256, 80, 80]          16,384\n",
      "      BatchNorm2d-34          [32, 256, 80, 80]             512\n",
      "             ReLU-35          [32, 256, 80, 80]               0\n",
      "       Bottleneck-36          [32, 256, 80, 80]               0\n",
      "           Conv2d-37          [32, 128, 80, 80]          32,768\n",
      "      BatchNorm2d-38          [32, 128, 80, 80]             256\n",
      "             ReLU-39          [32, 128, 80, 80]               0\n",
      "           Conv2d-40          [32, 128, 40, 40]         147,456\n",
      "      BatchNorm2d-41          [32, 128, 40, 40]             256\n",
      "             ReLU-42          [32, 128, 40, 40]               0\n",
      "           Conv2d-43          [32, 512, 40, 40]          65,536\n",
      "      BatchNorm2d-44          [32, 512, 40, 40]           1,024\n",
      "           Conv2d-45          [32, 512, 40, 40]         131,072\n",
      "      BatchNorm2d-46          [32, 512, 40, 40]           1,024\n",
      "             ReLU-47          [32, 512, 40, 40]               0\n",
      "       Bottleneck-48          [32, 512, 40, 40]               0\n",
      "           Conv2d-49          [32, 128, 40, 40]          65,536\n",
      "      BatchNorm2d-50          [32, 128, 40, 40]             256\n",
      "             ReLU-51          [32, 128, 40, 40]               0\n",
      "           Conv2d-52          [32, 128, 40, 40]         147,456\n",
      "      BatchNorm2d-53          [32, 128, 40, 40]             256\n",
      "             ReLU-54          [32, 128, 40, 40]               0\n",
      "           Conv2d-55          [32, 512, 40, 40]          65,536\n",
      "      BatchNorm2d-56          [32, 512, 40, 40]           1,024\n",
      "             ReLU-57          [32, 512, 40, 40]               0\n",
      "       Bottleneck-58          [32, 512, 40, 40]               0\n",
      "           Conv2d-59          [32, 128, 40, 40]          65,536\n",
      "      BatchNorm2d-60          [32, 128, 40, 40]             256\n",
      "             ReLU-61          [32, 128, 40, 40]               0\n",
      "           Conv2d-62          [32, 128, 40, 40]         147,456\n",
      "      BatchNorm2d-63          [32, 128, 40, 40]             256\n",
      "             ReLU-64          [32, 128, 40, 40]               0\n",
      "           Conv2d-65          [32, 512, 40, 40]          65,536\n",
      "      BatchNorm2d-66          [32, 512, 40, 40]           1,024\n",
      "             ReLU-67          [32, 512, 40, 40]               0\n",
      "       Bottleneck-68          [32, 512, 40, 40]               0\n",
      "           Conv2d-69          [32, 128, 40, 40]          65,536\n",
      "      BatchNorm2d-70          [32, 128, 40, 40]             256\n",
      "             ReLU-71          [32, 128, 40, 40]               0\n",
      "           Conv2d-72          [32, 128, 40, 40]         147,456\n",
      "      BatchNorm2d-73          [32, 128, 40, 40]             256\n",
      "             ReLU-74          [32, 128, 40, 40]               0\n",
      "           Conv2d-75          [32, 512, 40, 40]          65,536\n",
      "      BatchNorm2d-76          [32, 512, 40, 40]           1,024\n",
      "             ReLU-77          [32, 512, 40, 40]               0\n",
      "       Bottleneck-78          [32, 512, 40, 40]               0\n",
      "           Conv2d-79          [32, 256, 40, 40]         131,072\n",
      "      BatchNorm2d-80          [32, 256, 40, 40]             512\n",
      "             ReLU-81          [32, 256, 40, 40]               0\n",
      "           Conv2d-82          [32, 256, 20, 20]         589,824\n",
      "      BatchNorm2d-83          [32, 256, 20, 20]             512\n",
      "             ReLU-84          [32, 256, 20, 20]               0\n",
      "           Conv2d-85         [32, 1024, 20, 20]         262,144\n",
      "      BatchNorm2d-86         [32, 1024, 20, 20]           2,048\n",
      "           Conv2d-87         [32, 1024, 20, 20]         524,288\n",
      "      BatchNorm2d-88         [32, 1024, 20, 20]           2,048\n",
      "             ReLU-89         [32, 1024, 20, 20]               0\n",
      "       Bottleneck-90         [32, 1024, 20, 20]               0\n",
      "           Conv2d-91          [32, 256, 20, 20]         262,144\n",
      "      BatchNorm2d-92          [32, 256, 20, 20]             512\n",
      "             ReLU-93          [32, 256, 20, 20]               0\n",
      "           Conv2d-94          [32, 256, 20, 20]         589,824\n",
      "      BatchNorm2d-95          [32, 256, 20, 20]             512\n",
      "             ReLU-96          [32, 256, 20, 20]               0\n",
      "           Conv2d-97         [32, 1024, 20, 20]         262,144\n",
      "      BatchNorm2d-98         [32, 1024, 20, 20]           2,048\n",
      "             ReLU-99         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-100         [32, 1024, 20, 20]               0\n",
      "          Conv2d-101          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-102          [32, 256, 20, 20]             512\n",
      "            ReLU-103          [32, 256, 20, 20]               0\n",
      "          Conv2d-104          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-105          [32, 256, 20, 20]             512\n",
      "            ReLU-106          [32, 256, 20, 20]               0\n",
      "          Conv2d-107         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-108         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-109         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-110         [32, 1024, 20, 20]               0\n",
      "          Conv2d-111          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-112          [32, 256, 20, 20]             512\n",
      "            ReLU-113          [32, 256, 20, 20]               0\n",
      "          Conv2d-114          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-115          [32, 256, 20, 20]             512\n",
      "            ReLU-116          [32, 256, 20, 20]               0\n",
      "          Conv2d-117         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-118         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-119         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-120         [32, 1024, 20, 20]               0\n",
      "          Conv2d-121          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-122          [32, 256, 20, 20]             512\n",
      "            ReLU-123          [32, 256, 20, 20]               0\n",
      "          Conv2d-124          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-125          [32, 256, 20, 20]             512\n",
      "            ReLU-126          [32, 256, 20, 20]               0\n",
      "          Conv2d-127         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-128         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-129         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-130         [32, 1024, 20, 20]               0\n",
      "          Conv2d-131          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-132          [32, 256, 20, 20]             512\n",
      "            ReLU-133          [32, 256, 20, 20]               0\n",
      "          Conv2d-134          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-135          [32, 256, 20, 20]             512\n",
      "            ReLU-136          [32, 256, 20, 20]               0\n",
      "          Conv2d-137         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-138         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-139         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-140         [32, 1024, 20, 20]               0\n",
      "          Conv2d-141          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-142          [32, 256, 20, 20]             512\n",
      "            ReLU-143          [32, 256, 20, 20]               0\n",
      "          Conv2d-144          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-145          [32, 256, 20, 20]             512\n",
      "            ReLU-146          [32, 256, 20, 20]               0\n",
      "          Conv2d-147         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-148         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-149         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-150         [32, 1024, 20, 20]               0\n",
      "          Conv2d-151          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-152          [32, 256, 20, 20]             512\n",
      "            ReLU-153          [32, 256, 20, 20]               0\n",
      "          Conv2d-154          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-155          [32, 256, 20, 20]             512\n",
      "            ReLU-156          [32, 256, 20, 20]               0\n",
      "          Conv2d-157         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-158         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-159         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-160         [32, 1024, 20, 20]               0\n",
      "          Conv2d-161          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-162          [32, 256, 20, 20]             512\n",
      "            ReLU-163          [32, 256, 20, 20]               0\n",
      "          Conv2d-164          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-165          [32, 256, 20, 20]             512\n",
      "            ReLU-166          [32, 256, 20, 20]               0\n",
      "          Conv2d-167         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-168         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-169         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-170         [32, 1024, 20, 20]               0\n",
      "          Conv2d-171          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-172          [32, 256, 20, 20]             512\n",
      "            ReLU-173          [32, 256, 20, 20]               0\n",
      "          Conv2d-174          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-175          [32, 256, 20, 20]             512\n",
      "            ReLU-176          [32, 256, 20, 20]               0\n",
      "          Conv2d-177         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-178         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-179         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-180         [32, 1024, 20, 20]               0\n",
      "          Conv2d-181          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-182          [32, 256, 20, 20]             512\n",
      "            ReLU-183          [32, 256, 20, 20]               0\n",
      "          Conv2d-184          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-185          [32, 256, 20, 20]             512\n",
      "            ReLU-186          [32, 256, 20, 20]               0\n",
      "          Conv2d-187         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-188         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-189         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-190         [32, 1024, 20, 20]               0\n",
      "          Conv2d-191          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-192          [32, 256, 20, 20]             512\n",
      "            ReLU-193          [32, 256, 20, 20]               0\n",
      "          Conv2d-194          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-195          [32, 256, 20, 20]             512\n",
      "            ReLU-196          [32, 256, 20, 20]               0\n",
      "          Conv2d-197         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-198         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-199         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-200         [32, 1024, 20, 20]               0\n",
      "          Conv2d-201          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-202          [32, 256, 20, 20]             512\n",
      "            ReLU-203          [32, 256, 20, 20]               0\n",
      "          Conv2d-204          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-205          [32, 256, 20, 20]             512\n",
      "            ReLU-206          [32, 256, 20, 20]               0\n",
      "          Conv2d-207         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-208         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-209         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-210         [32, 1024, 20, 20]               0\n",
      "          Conv2d-211          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-212          [32, 256, 20, 20]             512\n",
      "            ReLU-213          [32, 256, 20, 20]               0\n",
      "          Conv2d-214          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-215          [32, 256, 20, 20]             512\n",
      "            ReLU-216          [32, 256, 20, 20]               0\n",
      "          Conv2d-217         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-218         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-219         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-220         [32, 1024, 20, 20]               0\n",
      "          Conv2d-221          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-222          [32, 256, 20, 20]             512\n",
      "            ReLU-223          [32, 256, 20, 20]               0\n",
      "          Conv2d-224          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-225          [32, 256, 20, 20]             512\n",
      "            ReLU-226          [32, 256, 20, 20]               0\n",
      "          Conv2d-227         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-228         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-229         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-230         [32, 1024, 20, 20]               0\n",
      "          Conv2d-231          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-232          [32, 256, 20, 20]             512\n",
      "            ReLU-233          [32, 256, 20, 20]               0\n",
      "          Conv2d-234          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-235          [32, 256, 20, 20]             512\n",
      "            ReLU-236          [32, 256, 20, 20]               0\n",
      "          Conv2d-237         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-238         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-239         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-240         [32, 1024, 20, 20]               0\n",
      "          Conv2d-241          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-242          [32, 256, 20, 20]             512\n",
      "            ReLU-243          [32, 256, 20, 20]               0\n",
      "          Conv2d-244          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-245          [32, 256, 20, 20]             512\n",
      "            ReLU-246          [32, 256, 20, 20]               0\n",
      "          Conv2d-247         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-248         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-249         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-250         [32, 1024, 20, 20]               0\n",
      "          Conv2d-251          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-252          [32, 256, 20, 20]             512\n",
      "            ReLU-253          [32, 256, 20, 20]               0\n",
      "          Conv2d-254          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-255          [32, 256, 20, 20]             512\n",
      "            ReLU-256          [32, 256, 20, 20]               0\n",
      "          Conv2d-257         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-258         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-259         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-260         [32, 1024, 20, 20]               0\n",
      "          Conv2d-261          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-262          [32, 256, 20, 20]             512\n",
      "            ReLU-263          [32, 256, 20, 20]               0\n",
      "          Conv2d-264          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-265          [32, 256, 20, 20]             512\n",
      "            ReLU-266          [32, 256, 20, 20]               0\n",
      "          Conv2d-267         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-268         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-269         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-270         [32, 1024, 20, 20]               0\n",
      "          Conv2d-271          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-272          [32, 256, 20, 20]             512\n",
      "            ReLU-273          [32, 256, 20, 20]               0\n",
      "          Conv2d-274          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-275          [32, 256, 20, 20]             512\n",
      "            ReLU-276          [32, 256, 20, 20]               0\n",
      "          Conv2d-277         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-278         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-279         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-280         [32, 1024, 20, 20]               0\n",
      "          Conv2d-281          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-282          [32, 256, 20, 20]             512\n",
      "            ReLU-283          [32, 256, 20, 20]               0\n",
      "          Conv2d-284          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-285          [32, 256, 20, 20]             512\n",
      "            ReLU-286          [32, 256, 20, 20]               0\n",
      "          Conv2d-287         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-288         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-289         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-290         [32, 1024, 20, 20]               0\n",
      "          Conv2d-291          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-292          [32, 256, 20, 20]             512\n",
      "            ReLU-293          [32, 256, 20, 20]               0\n",
      "          Conv2d-294          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-295          [32, 256, 20, 20]             512\n",
      "            ReLU-296          [32, 256, 20, 20]               0\n",
      "          Conv2d-297         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-298         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-299         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-300         [32, 1024, 20, 20]               0\n",
      "          Conv2d-301          [32, 256, 20, 20]         262,144\n",
      "     BatchNorm2d-302          [32, 256, 20, 20]             512\n",
      "            ReLU-303          [32, 256, 20, 20]               0\n",
      "          Conv2d-304          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-305          [32, 256, 20, 20]             512\n",
      "            ReLU-306          [32, 256, 20, 20]               0\n",
      "          Conv2d-307         [32, 1024, 20, 20]         262,144\n",
      "     BatchNorm2d-308         [32, 1024, 20, 20]           2,048\n",
      "            ReLU-309         [32, 1024, 20, 20]               0\n",
      "      Bottleneck-310         [32, 1024, 20, 20]               0\n",
      "          Conv2d-311          [32, 512, 20, 20]         524,288\n",
      "     BatchNorm2d-312          [32, 512, 20, 20]           1,024\n",
      "            ReLU-313          [32, 512, 20, 20]               0\n",
      "          Conv2d-314          [32, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-315          [32, 512, 10, 10]           1,024\n",
      "            ReLU-316          [32, 512, 10, 10]               0\n",
      "          Conv2d-317         [32, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-318         [32, 2048, 10, 10]           4,096\n",
      "          Conv2d-319         [32, 2048, 10, 10]       2,097,152\n",
      "     BatchNorm2d-320         [32, 2048, 10, 10]           4,096\n",
      "            ReLU-321         [32, 2048, 10, 10]               0\n",
      "      Bottleneck-322         [32, 2048, 10, 10]               0\n",
      "          Conv2d-323          [32, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-324          [32, 512, 10, 10]           1,024\n",
      "            ReLU-325          [32, 512, 10, 10]               0\n",
      "          Conv2d-326          [32, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-327          [32, 512, 10, 10]           1,024\n",
      "            ReLU-328          [32, 512, 10, 10]               0\n",
      "          Conv2d-329         [32, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-330         [32, 2048, 10, 10]           4,096\n",
      "            ReLU-331         [32, 2048, 10, 10]               0\n",
      "      Bottleneck-332         [32, 2048, 10, 10]               0\n",
      "          Conv2d-333          [32, 512, 10, 10]       1,048,576\n",
      "     BatchNorm2d-334          [32, 512, 10, 10]           1,024\n",
      "            ReLU-335          [32, 512, 10, 10]               0\n",
      "          Conv2d-336          [32, 512, 10, 10]       2,359,296\n",
      "     BatchNorm2d-337          [32, 512, 10, 10]           1,024\n",
      "            ReLU-338          [32, 512, 10, 10]               0\n",
      "          Conv2d-339         [32, 2048, 10, 10]       1,048,576\n",
      "     BatchNorm2d-340         [32, 2048, 10, 10]           4,096\n",
      "            ReLU-341         [32, 2048, 10, 10]               0\n",
      "      Bottleneck-342         [32, 2048, 10, 10]               0\n",
      "   ResNetEncoder-343  [[-1, 3, 320, 320], [-1, 64, 160, 160], [-1, 256, 80, 80], [-1, 512, 40, 40], [-1, 1024, 20, 20], [-1, 2048, 10, 10]]               0\n",
      "        Identity-344         [32, 2048, 10, 10]               0\n",
      "        Identity-345         [32, 3072, 20, 20]               0\n",
      "       Attention-346         [32, 3072, 20, 20]               0\n",
      "          Conv2d-347          [32, 256, 20, 20]       7,077,888\n",
      "     BatchNorm2d-348          [32, 256, 20, 20]             512\n",
      "            ReLU-349          [32, 256, 20, 20]               0\n",
      "          Conv2d-350          [32, 256, 20, 20]         589,824\n",
      "     BatchNorm2d-351          [32, 256, 20, 20]             512\n",
      "            ReLU-352          [32, 256, 20, 20]               0\n",
      "        Identity-353          [32, 256, 20, 20]               0\n",
      "       Attention-354          [32, 256, 20, 20]               0\n",
      "    DecoderBlock-355          [32, 256, 20, 20]               0\n",
      "        Identity-356          [32, 768, 40, 40]               0\n",
      "       Attention-357          [32, 768, 40, 40]               0\n",
      "          Conv2d-358          [32, 128, 40, 40]         884,736\n",
      "     BatchNorm2d-359          [32, 128, 40, 40]             256\n",
      "            ReLU-360          [32, 128, 40, 40]               0\n",
      "          Conv2d-361          [32, 128, 40, 40]         147,456\n",
      "     BatchNorm2d-362          [32, 128, 40, 40]             256\n",
      "            ReLU-363          [32, 128, 40, 40]               0\n",
      "        Identity-364          [32, 128, 40, 40]               0\n",
      "       Attention-365          [32, 128, 40, 40]               0\n",
      "    DecoderBlock-366          [32, 128, 40, 40]               0\n",
      "        Identity-367          [32, 384, 80, 80]               0\n",
      "       Attention-368          [32, 384, 80, 80]               0\n",
      "          Conv2d-369           [32, 64, 80, 80]         221,184\n",
      "     BatchNorm2d-370           [32, 64, 80, 80]             128\n",
      "            ReLU-371           [32, 64, 80, 80]               0\n",
      "          Conv2d-372           [32, 64, 80, 80]          36,864\n",
      "     BatchNorm2d-373           [32, 64, 80, 80]             128\n",
      "            ReLU-374           [32, 64, 80, 80]               0\n",
      "        Identity-375           [32, 64, 80, 80]               0\n",
      "       Attention-376           [32, 64, 80, 80]               0\n",
      "    DecoderBlock-377           [32, 64, 80, 80]               0\n",
      "        Identity-378        [32, 128, 160, 160]               0\n",
      "       Attention-379        [32, 128, 160, 160]               0\n",
      "          Conv2d-380         [32, 32, 160, 160]          36,864\n",
      "     BatchNorm2d-381         [32, 32, 160, 160]              64\n",
      "            ReLU-382         [32, 32, 160, 160]               0\n",
      "          Conv2d-383         [32, 32, 160, 160]           9,216\n",
      "     BatchNorm2d-384         [32, 32, 160, 160]              64\n",
      "            ReLU-385         [32, 32, 160, 160]               0\n",
      "        Identity-386         [32, 32, 160, 160]               0\n",
      "       Attention-387         [32, 32, 160, 160]               0\n",
      "    DecoderBlock-388         [32, 32, 160, 160]               0\n",
      "          Conv2d-389         [32, 16, 320, 320]           4,608\n",
      "     BatchNorm2d-390         [32, 16, 320, 320]              32\n",
      "            ReLU-391         [32, 16, 320, 320]               0\n",
      "          Conv2d-392         [32, 16, 320, 320]           2,304\n",
      "     BatchNorm2d-393         [32, 16, 320, 320]              32\n",
      "            ReLU-394         [32, 16, 320, 320]               0\n",
      "        Identity-395         [32, 16, 320, 320]               0\n",
      "       Attention-396         [32, 16, 320, 320]               0\n",
      "    DecoderBlock-397         [32, 16, 320, 320]               0\n",
      "     UnetDecoder-398         [32, 16, 320, 320]               0\n",
      "          Conv2d-399          [32, 1, 320, 320]             145\n",
      "        Identity-400          [32, 1, 320, 320]               0\n",
      "         Sigmoid-401          [32, 1, 320, 320]               0\n",
      "      Activation-402          [32, 1, 320, 320]               0\n",
      "================================================================\n",
      "Total params: 51,513,233\n",
      "Trainable params: 51,513,233\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 37.50\n",
      "Forward/backward pass size (MB): 6819.50\n",
      "Params size (MB): 196.51\n",
      "Estimated Total Size (MB): 7053.51\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lm3088\\Anaconda3\\envs\\venv\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,320,320), batch_size=BATCH_SIZE, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24cc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCS(Metric):\n",
    "    __name__ = 'DCS'\n",
    "\n",
    "    def __init__(self, eps=0.00001, activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "     \n",
    "        self.activation = Activation(activation)\n",
    "        self.ignore_channels = ignore_channels\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        y_pr = self.activation(y_pr)\n",
    "        dice_numerator = 2 * torch.sum(y_pr * y_gt) + self.eps\n",
    "        dice_denominator = torch.sum(y_pr) + torch.sum(y_gt) + self.eps\n",
    "        dice_coefficient = dice_numerator / dice_denominator\n",
    "        return dice_coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc13578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lm3088\\Anaconda3\\envs\\venv\\lib\\site-packages\\albumentations\\augmentations\\transforms.py:1802: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(\n",
    "    df_train['images'], \n",
    "    df_train['masks'], \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    df_val['images'], \n",
    "    df_val['masks'], \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b292052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "    DCS()\n",
    "    \n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b3687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e680cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 125 epochs\n",
    "EPOCHS=125\n",
    "if os.path.exists(WEIGHTS_PATH)==False:\n",
    "    os.mkdir(WEIGHTS_PATH)\n",
    "else:\n",
    "    print(f\"Warning! Directory {WEIGHTS_PATH } already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9582a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:55<00:00,  3.60it/s, dice_loss - 0.305, iou_score - 0.6087, DCS - 0.695]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:51<00:00, 30.26it/s, dice_loss - 0.4845, iou_score - 0.7992, DCS - 0.5049]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:29<00:00,  3.73it/s, dice_loss - 0.1606, iou_score - 0.7372, DCS - 0.8394]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:37<00:00, 32.87it/s, dice_loss - 0.4024, iou_score - 0.8223, DCS - 0.531]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:56<00:00,  3.59it/s, dice_loss - 0.1372, iou_score - 0.7707, DCS - 0.8628]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:59<00:00, 29.00it/s, dice_loss - 0.3334, iou_score - 0.844, DCS - 0.5328]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:14<00:00,  3.82it/s, dice_loss - 0.1237, iou_score - 0.7896, DCS - 0.8763]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.91it/s, dice_loss - 0.2194, iou_score - 0.8029, DCS - 0.5337]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:10<00:00,  3.84it/s, dice_loss - 0.1134, iou_score - 0.8058, DCS - 0.8866]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.84it/s, dice_loss - 0.1761, iou_score - 0.8308, DCS - 0.5352]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:15<00:00,  3.81it/s, dice_loss - 0.1117, iou_score - 0.8082, DCS - 0.8883]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.22it/s, dice_loss - 0.2116, iou_score - 0.7387, DCS - 0.4895]\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:13<00:00,  3.82it/s, dice_loss - 0.1038, iou_score - 0.8184, DCS - 0.8962]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:29<00:00, 34.68it/s, dice_loss - 0.123, iou_score - 0.845, DCS - 0.5396]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:12<00:00,  3.83it/s, dice_loss - 0.09889, iou_score - 0.8269, DCS - 0.9011]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.94it/s, dice_loss - 0.1373, iou_score - 0.84, DCS - 0.5481]\n",
      "\n",
      "Epoch: 8\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:15<00:00,  3.81it/s, dice_loss - 0.09554, iou_score - 0.8325, DCS - 0.9045]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.28it/s, dice_loss - 0.2384, iou_score - 0.7142, DCS - 0.5313]\n",
      "\n",
      "Epoch: 9\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.1013, iou_score - 0.824, DCS - 0.8987]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.81it/s, dice_loss - 0.1187, iou_score - 0.8497, DCS - 0.5494]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 10\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.09453, iou_score - 0.8338, DCS - 0.9054]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:35<00:00, 33.40it/s, dice_loss - 0.1002, iou_score - 0.8649, DCS - 0.5529]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 11\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.09134, iou_score - 0.8387, DCS - 0.9082]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:32<00:00, 34.05it/s, dice_loss - 0.1294, iou_score - 0.8301, DCS - 0.5495]\n",
      "\n",
      "Epoch: 12\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.09042, iou_score - 0.842, DCS - 0.9094]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.93it/s, dice_loss - 0.09258, iou_score - 0.87, DCS - 0.5455]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 13\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.79it/s, dice_loss - 0.09296, iou_score - 0.8383, DCS - 0.9064]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:32<00:00, 34.08it/s, dice_loss - 0.09102, iou_score - 0.87, DCS - 0.5523]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 14\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.09171, iou_score - 0.8394, DCS - 0.9081]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:36<00:00, 33.28it/s, dice_loss - 0.08488, iou_score - 0.8746, DCS - 0.5508]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 15\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.78it/s, dice_loss - 0.08644, iou_score - 0.8478, DCS - 0.913]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.17it/s, dice_loss - 0.09134, iou_score - 0.8707, DCS - 0.5515]\n",
      "\n",
      "Epoch: 16\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.08081, iou_score - 0.8552, DCS - 0.9192]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.86it/s, dice_loss - 0.09596, iou_score - 0.8583, DCS - 0.546]\n",
      "\n",
      "Epoch: 17\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.08106, iou_score - 0.8552, DCS - 0.9189]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.37it/s, dice_loss - 0.09402, iou_score - 0.8672, DCS - 0.5528]\n",
      "\n",
      "Epoch: 18\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.08274, iou_score - 0.8533, DCS - 0.9165]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.51it/s, dice_loss - 0.09022, iou_score - 0.8685, DCS - 0.5603]\n",
      "\n",
      "Epoch: 19\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.08088, iou_score - 0.8556, DCS - 0.9187]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:34<00:00, 33.59it/s, dice_loss - 0.09161, iou_score - 0.8657, DCS - 0.558]\n",
      "\n",
      "Epoch: 20\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.08156, iou_score - 0.8548, DCS - 0.9184]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:34<00:00, 33.51it/s, dice_loss - 0.09294, iou_score - 0.8658, DCS - 0.5541]\n",
      "\n",
      "Epoch: 21\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.07824, iou_score - 0.8601, DCS - 0.9212]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:37<00:00, 32.95it/s, dice_loss - 0.09305, iou_score - 0.8665, DCS - 0.5536]\n",
      "\n",
      "Epoch: 22\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.07533, iou_score - 0.865, DCS - 0.9243]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:36<00:00, 33.11it/s, dice_loss - 0.08493, iou_score - 0.8731, DCS - 0.5585]\n",
      "\n",
      "Epoch: 23\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.07621, iou_score - 0.8632, DCS - 0.9215]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:36<00:00, 33.11it/s, dice_loss - 0.09313, iou_score - 0.8648, DCS - 0.5764]\n",
      "\n",
      "Epoch: 24\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.07161, iou_score - 0.8706, DCS - 0.928]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:37<00:00, 32.96it/s, dice_loss - 0.08399, iou_score - 0.8732, DCS - 0.5694]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 25\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.07469, iou_score - 0.8663, DCS - 0.925]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:37<00:00, 33.06it/s, dice_loss - 0.1003, iou_score - 0.8587, DCS - 0.5465]\n",
      "Decrease decoder learning rate to 1e-5!\n",
      "\n",
      "Epoch: 26\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:15<00:00,  3.81it/s, dice_loss - 0.0734, iou_score - 0.867, DCS - 0.9266]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.86it/s, dice_loss - 0.08747, iou_score - 0.8732, DCS - 0.5574]\n",
      "\n",
      "Epoch: 27\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:12<00:00,  3.83it/s, dice_loss - 0.06966, iou_score - 0.8741, DCS - 0.9286]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.85it/s, dice_loss - 0.08432, iou_score - 0.875, DCS - 0.5642]\n",
      "\n",
      "Epoch: 28\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:11<00:00,  3.84it/s, dice_loss - 0.06503, iou_score - 0.8813, DCS - 0.9323]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.90it/s, dice_loss - 0.08283, iou_score - 0.8768, DCS - 0.5674]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 29\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:13<00:00,  3.82it/s, dice_loss - 0.06234, iou_score - 0.8859, DCS - 0.9353]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.92it/s, dice_loss - 0.08528, iou_score - 0.8742, DCS - 0.5694]\n",
      "\n",
      "Epoch: 30\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:12<00:00,  3.83it/s, dice_loss - 0.0613, iou_score - 0.8875, DCS - 0.9379]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:33<00:00, 33.80it/s, dice_loss - 0.08236, iou_score - 0.8772, DCS - 0.5712]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 31\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:13<00:00,  3.82it/s, dice_loss - 0.06158, iou_score - 0.8878, DCS - 0.9369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.16it/s, dice_loss - 0.07829, iou_score - 0.8811, DCS - 0.5739]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 32\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:16<00:00,  3.81it/s, dice_loss - 0.06043, iou_score - 0.8888, DCS - 0.9392]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.22it/s, dice_loss - 0.07923, iou_score - 0.8798, DCS - 0.5701]\n",
      "\n",
      "Epoch: 33\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:16<00:00,  3.81it/s, dice_loss - 0.06335, iou_score - 0.886, DCS - 0.9359]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.27it/s, dice_loss - 0.08198, iou_score - 0.8772, DCS - 0.5748]\n",
      "\n",
      "Epoch: 34\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:16<00:00,  3.80it/s, dice_loss - 0.05807, iou_score - 0.8925, DCS - 0.9407]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.22it/s, dice_loss - 0.08004, iou_score - 0.879, DCS - 0.5761]\n",
      "\n",
      "Epoch: 35\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:16<00:00,  3.81it/s, dice_loss - 0.05802, iou_score - 0.8931, DCS - 0.9416]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.03it/s, dice_loss - 0.07899, iou_score - 0.8801, DCS - 0.5783]\n",
      "\n",
      "Epoch: 36\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:15<00:00,  3.81it/s, dice_loss - 0.05759, iou_score - 0.8944, DCS - 0.9409]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.16it/s, dice_loss - 0.07805, iou_score - 0.8814, DCS - 0.5756]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 37\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:17<00:00,  3.80it/s, dice_loss - 0.05715, iou_score - 0.8948, DCS - 0.9417]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.11it/s, dice_loss - 0.07848, iou_score - 0.8809, DCS - 0.5728]\n",
      "\n",
      "Epoch: 38\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.80it/s, dice_loss - 0.05606, iou_score - 0.8959, DCS - 0.9436]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.06it/s, dice_loss - 0.07692, iou_score - 0.8824, DCS - 0.5763]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 39\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05556, iou_score - 0.8973, DCS - 0.9418]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.14it/s, dice_loss - 0.08075, iou_score - 0.8781, DCS - 0.5795]\n",
      "\n",
      "Epoch: 40\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05339, iou_score - 0.9005, DCS - 0.9455]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.07it/s, dice_loss - 0.08086, iou_score - 0.8782, DCS - 0.5834]\n",
      "\n",
      "Epoch: 41\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.055, iou_score - 0.8982, DCS - 0.9438]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.92it/s, dice_loss - 0.07744, iou_score - 0.8817, DCS - 0.5911]\n",
      "\n",
      "Epoch: 42\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.05399, iou_score - 0.8996, DCS - 0.9433]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.24it/s, dice_loss - 0.08075, iou_score - 0.8783, DCS - 0.582]\n",
      "\n",
      "Epoch: 43\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:17<00:00,  3.80it/s, dice_loss - 0.05536, iou_score - 0.8981, DCS - 0.9438]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.13it/s, dice_loss - 0.07996, iou_score - 0.8788, DCS - 0.5838]\n",
      "\n",
      "Epoch: 44\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05388, iou_score - 0.9, DCS - 0.9446]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.24it/s, dice_loss - 0.08403, iou_score - 0.875, DCS - 0.5783]\n",
      "\n",
      "Epoch: 45\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.80it/s, dice_loss - 0.05496, iou_score - 0.8981, DCS - 0.9435]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.10it/s, dice_loss - 0.08094, iou_score - 0.8784, DCS - 0.5729]\n",
      "\n",
      "Epoch: 46\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:15<00:00,  3.81it/s, dice_loss - 0.05407, iou_score - 0.9004, DCS - 0.944]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.21it/s, dice_loss - 0.07795, iou_score - 0.8814, DCS - 0.5866]\n",
      "\n",
      "Epoch: 47\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.79it/s, dice_loss - 0.05426, iou_score - 0.9002, DCS - 0.9446]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.14it/s, dice_loss - 0.08158, iou_score - 0.8772, DCS - 0.5809]\n",
      "\n",
      "Epoch: 48\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.80it/s, dice_loss - 0.05398, iou_score - 0.9004, DCS - 0.9444]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.13it/s, dice_loss - 0.07591, iou_score - 0.8829, DCS - 0.5802]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 49\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05319, iou_score - 0.9014, DCS - 0.9457]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.21it/s, dice_loss - 0.07762, iou_score - 0.8816, DCS - 0.5831]\n",
      "\n",
      "Epoch: 50\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.79it/s, dice_loss - 0.05259, iou_score - 0.9021, DCS - 0.946]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.11it/s, dice_loss - 0.07819, iou_score - 0.8807, DCS - 0.5879]\n",
      "\n",
      "Epoch: 51\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.80it/s, dice_loss - 0.05047, iou_score - 0.9055, DCS - 0.948]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.19it/s, dice_loss - 0.07708, iou_score - 0.8822, DCS - 0.5915]\n",
      "\n",
      "Epoch: 52\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.05064, iou_score - 0.9053, DCS - 0.9482]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.04it/s, dice_loss - 0.07922, iou_score - 0.8794, DCS - 0.5731]\n",
      "\n",
      "Epoch: 53\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.80it/s, dice_loss - 0.05132, iou_score - 0.9046, DCS - 0.9467]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.23it/s, dice_loss - 0.07879, iou_score - 0.8803, DCS - 0.5887]\n",
      "\n",
      "Epoch: 54\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.05392, iou_score - 0.901, DCS - 0.9449]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.21it/s, dice_loss - 0.08007, iou_score - 0.8785, DCS - 0.5834]\n",
      "\n",
      "Epoch: 55\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.05433, iou_score - 0.9003, DCS - 0.9436]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.07it/s, dice_loss - 0.07871, iou_score - 0.8804, DCS - 0.5912]\n",
      "\n",
      "Epoch: 56\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05345, iou_score - 0.9017, DCS - 0.9454]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.18it/s, dice_loss - 0.08307, iou_score - 0.8758, DCS - 0.6023]\n",
      "\n",
      "Epoch: 57\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05126, iou_score - 0.9049, DCS - 0.948]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.14it/s, dice_loss - 0.0807, iou_score - 0.8772, DCS - 0.5982]\n",
      "\n",
      "Epoch: 58\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05105, iou_score - 0.9053, DCS - 0.947]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.18it/s, dice_loss - 0.07964, iou_score - 0.8786, DCS - 0.607]\n",
      "\n",
      "Epoch: 59\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05102, iou_score - 0.9052, DCS - 0.9486]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.09it/s, dice_loss - 0.0787, iou_score - 0.8804, DCS - 0.5872]\n",
      "\n",
      "Epoch: 60\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05083, iou_score - 0.9056, DCS - 0.9461]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.01it/s, dice_loss - 0.07654, iou_score - 0.8818, DCS - 0.5969]\n",
      "\n",
      "Epoch: 61\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.04965, iou_score - 0.9071, DCS - 0.9487]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.12it/s, dice_loss - 0.07628, iou_score - 0.8822, DCS - 0.5814]\n",
      "\n",
      "Epoch: 62\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.04844, iou_score - 0.9095, DCS - 0.9496]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.07it/s, dice_loss - 0.07635, iou_score - 0.8823, DCS - 0.5845]\n",
      "\n",
      "Epoch: 63\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05174, iou_score - 0.9049, DCS - 0.9464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.25it/s, dice_loss - 0.07628, iou_score - 0.8823, DCS - 0.6031]\n",
      "\n",
      "Epoch: 64\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:17<00:00,  3.80it/s, dice_loss - 0.05064, iou_score - 0.9066, DCS - 0.947]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.28it/s, dice_loss - 0.07715, iou_score - 0.8821, DCS - 0.5834]\n",
      "\n",
      "Epoch: 65\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.04912, iou_score - 0.9085, DCS - 0.9505]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.00it/s, dice_loss - 0.07694, iou_score - 0.8817, DCS - 0.5849]\n",
      "\n",
      "Epoch: 66\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:18<00:00,  3.79it/s, dice_loss - 0.04978, iou_score - 0.9076, DCS - 0.9479]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.18it/s, dice_loss - 0.07923, iou_score - 0.8797, DCS - 0.5814]\n",
      "\n",
      "Epoch: 67\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.05152, iou_score - 0.905, DCS - 0.9472]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.26it/s, dice_loss - 0.07645, iou_score - 0.8824, DCS - 0.5945]\n",
      "\n",
      "Epoch: 68\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04862, iou_score - 0.9091, DCS - 0.951]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.09it/s, dice_loss - 0.07739, iou_score - 0.8815, DCS - 0.5881]\n",
      "\n",
      "Epoch: 69\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04909, iou_score - 0.9085, DCS - 0.9482]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.05it/s, dice_loss - 0.07667, iou_score - 0.8821, DCS - 0.5892]\n",
      "\n",
      "Epoch: 70\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04894, iou_score - 0.9088, DCS - 0.9491]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.00it/s, dice_loss - 0.07624, iou_score - 0.8823, DCS - 0.5854]\n",
      "\n",
      "Epoch: 71\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.05068, iou_score - 0.907, DCS - 0.9478]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.90it/s, dice_loss - 0.07853, iou_score - 0.8796, DCS - 0.5928]\n",
      "\n",
      "Epoch: 72\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.78it/s, dice_loss - 0.04976, iou_score - 0.9078, DCS - 0.9491]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.53it/s, dice_loss - 0.07869, iou_score - 0.8806, DCS - 0.5954]\n",
      "\n",
      "Epoch: 73\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04784, iou_score - 0.9108, DCS - 0.9512]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.12it/s, dice_loss - 0.08172, iou_score - 0.8766, DCS - 0.5927]\n",
      "\n",
      "Epoch: 74\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.04824, iou_score - 0.9098, DCS - 0.951]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.00it/s, dice_loss - 0.07839, iou_score - 0.88, DCS - 0.5957]\n",
      "\n",
      "Epoch: 75\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04814, iou_score - 0.9106, DCS - 0.9511]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.99it/s, dice_loss - 0.07772, iou_score - 0.8809, DCS - 0.5979]\n",
      "\n",
      "Epoch: 76\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.04806, iou_score - 0.9104, DCS - 0.9511]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.02it/s, dice_loss - 0.07686, iou_score - 0.8819, DCS - 0.5972]\n",
      "\n",
      "Epoch: 77\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04829, iou_score - 0.9103, DCS - 0.9491]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.99it/s, dice_loss - 0.0814, iou_score - 0.8773, DCS - 0.5925]\n",
      "\n",
      "Epoch: 78\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.04828, iou_score - 0.9102, DCS - 0.9502]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.93it/s, dice_loss - 0.07962, iou_score - 0.8783, DCS - 0.5936]\n",
      "\n",
      "Epoch: 79\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.0457, iou_score - 0.914, DCS - 0.9528]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.12it/s, dice_loss - 0.07999, iou_score - 0.8787, DCS - 0.5968]\n",
      "\n",
      "Epoch: 80\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:17<00:00,  3.80it/s, dice_loss - 0.04776, iou_score - 0.9109, DCS - 0.9507]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.97it/s, dice_loss - 0.08315, iou_score - 0.8755, DCS - 0.5874]\n",
      "\n",
      "Epoch: 81\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.04951, iou_score - 0.9088, DCS - 0.9488]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.00it/s, dice_loss - 0.08084, iou_score - 0.8773, DCS - 0.5897]\n",
      "\n",
      "Epoch: 82\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04742, iou_score - 0.9114, DCS - 0.9506]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.05it/s, dice_loss - 0.07643, iou_score - 0.8825, DCS - 0.6073]\n",
      "\n",
      "Epoch: 83\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.04849, iou_score - 0.91, DCS - 0.9503]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 35.03it/s, dice_loss - 0.0796, iou_score - 0.8789, DCS - 0.6012]\n",
      "\n",
      "Epoch: 84\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.0475, iou_score - 0.9117, DCS - 0.9513]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.94it/s, dice_loss - 0.0771, iou_score - 0.8811, DCS - 0.6121]\n",
      "\n",
      "Epoch: 85\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.04796, iou_score - 0.9108, DCS - 0.9503]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:29<00:00, 34.84it/s, dice_loss - 0.07938, iou_score - 0.8791, DCS - 0.5996]\n",
      "\n",
      "Epoch: 86\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04693, iou_score - 0.9123, DCS - 0.9527]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.97it/s, dice_loss - 0.08021, iou_score - 0.8783, DCS - 0.6063]\n",
      "\n",
      "Epoch: 87\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:09<00:00,  3.84it/s, dice_loss - 0.04733, iou_score - 0.9122, DCS - 0.9507]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.18it/s, dice_loss - 0.07746, iou_score - 0.8811, DCS - 0.6045]\n",
      "\n",
      "Epoch: 88\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:09<00:00,  3.85it/s, dice_loss - 0.04691, iou_score - 0.9127, DCS - 0.9515]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.20it/s, dice_loss - 0.08188, iou_score - 0.8764, DCS - 0.5862]\n",
      "\n",
      "Epoch: 89\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:08<00:00,  3.85it/s, dice_loss - 0.04692, iou_score - 0.9122, DCS - 0.9508]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.26it/s, dice_loss - 0.08082, iou_score - 0.8777, DCS - 0.601]\n",
      "\n",
      "Epoch: 90\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:10<00:00,  3.84it/s, dice_loss - 0.04901, iou_score - 0.9101, DCS - 0.9502]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.13it/s, dice_loss - 0.07713, iou_score - 0.8812, DCS - 0.5928]\n",
      "\n",
      "Epoch: 91\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:07<00:00,  3.86it/s, dice_loss - 0.04662, iou_score - 0.9127, DCS - 0.9526]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.21it/s, dice_loss - 0.07733, iou_score - 0.8809, DCS - 0.5971]\n",
      "\n",
      "Epoch: 92\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:08<00:00,  3.85it/s, dice_loss - 0.04641, iou_score - 0.9132, DCS - 0.9524]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.26it/s, dice_loss - 0.08602, iou_score - 0.8726, DCS - 0.5883]\n",
      "\n",
      "Epoch: 93\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:08<00:00,  3.85it/s, dice_loss - 0.04747, iou_score - 0.9119, DCS - 0.951]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.21it/s, dice_loss - 0.07744, iou_score - 0.881, DCS - 0.5996]\n",
      "\n",
      "Epoch: 94\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:10<00:00,  3.84it/s, dice_loss - 0.04575, iou_score - 0.9144, DCS - 0.9531]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.26it/s, dice_loss - 0.07523, iou_score - 0.8833, DCS - 0.5946]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 95\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:08<00:00,  3.85it/s, dice_loss - 0.04772, iou_score - 0.9116, DCS - 0.9515]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:27<00:00, 35.19it/s, dice_loss - 0.07738, iou_score - 0.8808, DCS - 0.5937]\n",
      "\n",
      "Epoch: 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:10<00:00,  3.84it/s, dice_loss - 0.04528, iou_score - 0.9149, DCS - 0.9528]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:26<00:00, 35.38it/s, dice_loss - 0.07614, iou_score - 0.8824, DCS - 0.6227]\n",
      "\n",
      "Epoch: 97\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:12<00:00,  3.83it/s, dice_loss - 0.04584, iou_score - 0.9147, DCS - 0.9538]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:26<00:00, 35.32it/s, dice_loss - 0.07919, iou_score - 0.8794, DCS - 0.5985]\n",
      "\n",
      "Epoch: 98\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:13<00:00,  3.82it/s, dice_loss - 0.04588, iou_score - 0.9143, DCS - 0.9516]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:35<00:00, 33.46it/s, dice_loss - 0.0779, iou_score - 0.8811, DCS - 0.5959]\n",
      "\n",
      "Epoch: 99\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04413, iou_score - 0.9171, DCS - 0.9547]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:28<00:00, 34.98it/s, dice_loss - 0.07797, iou_score - 0.8806, DCS - 0.5964]\n",
      "\n",
      "Epoch: 100\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04609, iou_score - 0.914, DCS - 0.9528]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.54it/s, dice_loss - 0.07885, iou_score - 0.8797, DCS - 0.6036]\n",
      "\n",
      "Epoch: 101\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.78it/s, dice_loss - 0.04599, iou_score - 0.9142, DCS - 0.952]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.28it/s, dice_loss - 0.07453, iou_score - 0.8839, DCS - 0.5959]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 102\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.0452, iou_score - 0.9153, DCS - 0.954]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.41it/s, dice_loss - 0.07632, iou_score - 0.8822, DCS - 0.5924]\n",
      "\n",
      "Epoch: 103\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.79it/s, dice_loss - 0.04495, iou_score - 0.9158, DCS - 0.9528]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.42it/s, dice_loss - 0.0778, iou_score - 0.8809, DCS - 0.6034]\n",
      "\n",
      "Epoch: 104\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:20<00:00,  3.78it/s, dice_loss - 0.04395, iou_score - 0.9177, DCS - 0.9549]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.50it/s, dice_loss - 0.07865, iou_score - 0.8795, DCS - 0.5873]\n",
      "\n",
      "Epoch: 105\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:25<00:00,  3.76it/s, dice_loss - 0.04397, iou_score - 0.917, DCS - 0.9541]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.42it/s, dice_loss - 0.0766, iou_score - 0.8824, DCS - 0.6032]\n",
      "\n",
      "Epoch: 106\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04587, iou_score - 0.9147, DCS - 0.9523]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.39it/s, dice_loss - 0.0748, iou_score - 0.884, DCS - 0.592]\n",
      "\n",
      "Epoch: 107\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.78it/s, dice_loss - 0.04366, iou_score - 0.918, DCS - 0.9548]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.49it/s, dice_loss - 0.07865, iou_score - 0.8799, DCS - 0.6192]\n",
      "\n",
      "Epoch: 108\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04452, iou_score - 0.9169, DCS - 0.9543]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.18it/s, dice_loss - 0.07711, iou_score - 0.8816, DCS - 0.593]\n",
      "\n",
      "Epoch: 109\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:22<00:00,  3.77it/s, dice_loss - 0.04453, iou_score - 0.9168, DCS - 0.9543]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.32it/s, dice_loss - 0.07981, iou_score - 0.8789, DCS - 0.6128]\n",
      "\n",
      "Epoch: 110\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.04476, iou_score - 0.9166, DCS - 0.9537]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.18it/s, dice_loss - 0.07637, iou_score - 0.8825, DCS - 0.6188]\n",
      "\n",
      "Epoch: 111\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04424, iou_score - 0.9172, DCS - 0.9545]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:29<00:00, 34.66it/s, dice_loss - 0.07536, iou_score - 0.8827, DCS - 0.6181]\n",
      "\n",
      "Epoch: 112\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:25<00:00,  3.76it/s, dice_loss - 0.04477, iou_score - 0.9162, DCS - 0.9541]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.50it/s, dice_loss - 0.07564, iou_score - 0.8828, DCS - 0.6242]\n",
      "\n",
      "Epoch: 113\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04376, iou_score - 0.9177, DCS - 0.9555]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.43it/s, dice_loss - 0.07604, iou_score - 0.8824, DCS - 0.6076]\n",
      "\n",
      "Epoch: 114\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.04531, iou_score - 0.9158, DCS - 0.9543]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:34<00:00, 33.60it/s, dice_loss - 0.07897, iou_score - 0.8797, DCS - 0.6202]\n",
      "\n",
      "Epoch: 115\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:19<00:00,  3.79it/s, dice_loss - 0.04367, iou_score - 0.9181, DCS - 0.9536]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:35<00:00, 33.48it/s, dice_loss - 0.07744, iou_score - 0.8809, DCS - 0.6028]\n",
      "\n",
      "Epoch: 116\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:25<00:00,  3.76it/s, dice_loss - 0.04344, iou_score - 0.9187, DCS - 0.9547]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:34<00:00, 33.54it/s, dice_loss - 0.07703, iou_score - 0.8818, DCS - 0.6252]\n",
      "\n",
      "Epoch: 117\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.0444, iou_score - 0.9172, DCS - 0.9537]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.60it/s, dice_loss - 0.07722, iou_score - 0.8811, DCS - 0.6053]\n",
      "\n",
      "Epoch: 118\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04305, iou_score - 0.9189, DCS - 0.955]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:32<00:00, 34.14it/s, dice_loss - 0.07789, iou_score - 0.8815, DCS - 0.5911]\n",
      "\n",
      "Epoch: 119\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:21<00:00,  3.78it/s, dice_loss - 0.04319, iou_score - 0.9194, DCS - 0.956]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.18it/s, dice_loss - 0.07657, iou_score - 0.8827, DCS - 0.601]\n",
      "\n",
      "Epoch: 120\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.04429, iou_score - 0.9171, DCS - 0.9549]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.60it/s, dice_loss - 0.07801, iou_score - 0.88, DCS - 0.5929]\n",
      "\n",
      "Epoch: 121\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.042, iou_score - 0.9206, DCS - 0.9561]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:29<00:00, 34.75it/s, dice_loss - 0.07759, iou_score - 0.8806, DCS - 0.6103]\n",
      "\n",
      "Epoch: 122\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04443, iou_score - 0.9177, DCS - 0.9532]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:30<00:00, 34.57it/s, dice_loss - 0.07494, iou_score - 0.8836, DCS - 0.6081]\n",
      "\n",
      "Epoch: 123\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:23<00:00,  3.77it/s, dice_loss - 0.04156, iou_score - 0.9213, DCS - 0.9562]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:31<00:00, 34.33it/s, dice_loss - 0.07666, iou_score - 0.8823, DCS - 0.6039]\n",
      "\n",
      "Epoch: 124\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2575/2575 [11:24<00:00,  3.76it/s, dice_loss - 0.04526, iou_score - 0.9167, DCS - 0.9532]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5192/5192 [02:35<00:00, 33.34it/s, dice_loss - 0.08571, iou_score - 0.873, DCS - 0.6111]\n"
     ]
    }
   ],
   "source": [
    "min_loss = 100000000\n",
    "train_history=defaultdict(list)\n",
    "valid_history=defaultdict(list)\n",
    "\n",
    "for i in range(0, EPOCHS):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if min_loss > valid_logs['dice_loss']:\n",
    "        min_loss = valid_logs['dice_loss']\n",
    "        torch.save(model, os.path.join(WEIGHTS_PATH,f'best_{str(i)}_{round(min_loss,4)}.pt'))\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n",
    "    # Maintain History\n",
    "    for log_key in train_logs.keys():\n",
    "        train_history[log_key].append(train_logs[log_key])\n",
    "        valid_history[log_key].append(valid_logs[log_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e41bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'dice_loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_history['dice_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"dice_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "106ace7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(valid_history).to_csv(os.path.join(WEIGHTS_PATH,'validation_logs.csv'))\n",
    "pd.DataFrame(train_history).to_csv(os.path.join(WEIGHTS_PATH,'train_logs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c44ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
